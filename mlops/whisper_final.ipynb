{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install evaluate -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_path = 'urls_normalized.tsv'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def load_tsv_file(tsv_path: str):\n",
        "    \"\"\"\n",
        "    –ü—Ä–æ—Å—Ç–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ TSV —Ñ–∞–π–ª–∞\n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        df = pd.read_csv(tsv_path, sep='\\t', encoding='utf-8')\n",
        "        \n",
        "        print(f\"‚úÖ TSV —Ñ–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ!\")\n",
        "        print(f\"üìä –†–∞–∑–º–µ—Ä: {df.shape[0]} —Å—Ç—Ä–æ–∫, {df.shape[1]} —Å—Ç–æ–ª–±—Ü–æ–≤\")\n",
        "        print(f\"üìã –°—Ç–æ–ª–±—Ü—ã: {list(df.columns)}\")\n",
        "        \n",
        "        return df\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ TSV: {e}\")\n",
        "       \n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
        "import torch\n",
        "import torchaudio\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "MODEL_NAME = \"openai/whisper-small\"\n",
        "\n",
        "data = load_tsv_file(file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for index, row in data.iterrows():\n",
        "    print(f\"{row}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "processor = WhisperProcessor.from_pretrained(MODEL_NAME)\n",
        "model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "model.to(device)\n",
        "\n",
        "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(\n",
        "    language=\"russian\", task=\"transcribe\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import tempfile\n",
        "\n",
        "results_learn_whisper_small = []\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "    url = row[0]\n",
        "    \n",
        "    wav, sr = torchaudio.load(url)\n",
        "    \n",
        "    # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ —Å–æ–æ—Ç–≤–µ—Å—Ç–≤—É–µ—Ç –ø–æ —á–∞—Å—Ç–æ—Ç–µ 16000 —Ç–æ resample –µ–µ –∫ —Ç–æ–π —á—Ç–æ –Ω—É–∂–Ω–æ\n",
        "    if sr != 16000:\n",
        "        wav = torchaudio.functional.resample(wav, orig_freq=sr, new_freq=16000)\n",
        "        sr = 16000\n",
        "    \n",
        "    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ç–µ–Ω–∑–æ—Ä –≤ numpy array –¥–ª—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞\n",
        "    audio_array = wav.squeeze().numpy()\n",
        "    \n",
        "    input_features = processor(\n",
        "        audio_array, \n",
        "        sampling_rate=sr, \n",
        "        return_tensors=\"pt\"\n",
        "    ).input_features.to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        predicted_ids = model.generate(input_features)\n",
        "    \n",
        "    transcription = processor.batch_decode(\n",
        "        predicted_ids, \n",
        "        skip_special_tokens=True\n",
        "    )[0]\n",
        "    \n",
        "    results_learn_whisper_small.append({\n",
        "        \"url\": url,\n",
        "        \"transcription\": transcription\n",
        "    })\n",
        "    \n",
        "    # —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–µ—Ä–≤—ã—Ö 10 —á—Ç–æ–±—ã –Ω–µ –∑–∞–±–∏–≤–∞—Ç—å log\n",
        "    if index >= 9:\n",
        "        print(f\"URL: {url}\")\n",
        "        print(f\"Text: {transcription}\")\n",
        "        print(\"---\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, T5TokenizerFast\n",
        "\n",
        "SPELL_MODEL_NAME = \"UrukHan/t5-russian-spell\"\n",
        "spell_tokenizer = T5TokenizerFast.from_pretrained(SPELL_MODEL_NAME)\n",
        "spell_model = AutoModelForSeq2SeqLM.from_pretrained(SPELL_MODEL_NAME)\n",
        "spell_model.to(device)\n",
        "\n",
        "MAX_INPUT = 256\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for result in results_learn_whisper_small[:10]:\n",
        "    original_text = result[\"transcription\"]\n",
        "\n",
        "    task_prefix = \"Spell correct: \"  \n",
        "    \n",
        "    encoded = spell_tokenizer(\n",
        "        [task_prefix + original_text],\n",
        "        padding=\"longest\",\n",
        "        max_length=MAX_INPUT,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = spell_model.generate(\n",
        "            encoded[\"input_ids\"].to(device), \n",
        "            max_length=MAX_INPUT,\n",
        "            early_stopping=True,\n",
        "            do_sample=False\n",
        "        )\n",
        "    \n",
        "    corrected_text = spell_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    print(f\"–û—Ä–∏–≥–∏–Ω–∞–ª: {original_text}\")\n",
        "    print(f\"–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: {corrected_text}\")\n",
        "    print(\"---\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = \"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from groq import Groq\n",
        "\n",
        "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spell_dataset = []\n",
        "\n",
        "prompt_text = \"\"\"\n",
        "–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π 50 –ø–∞—Ä \"—Ç–µ–∫—Å—Ç —Å –æ—à–∏–±–∫–∞–º–∏\" -> \"–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç\" –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.\n",
        "–ë–µ–∑ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è\n",
        "–û—à–∏–±–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ç–∏–ø–∏—á–Ω—ã–º–∏ –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏:\n",
        "- –∑–∞–º–µ–Ω–∞ –±—É–∫–≤ (–æ/–∞, –µ/–∏, —à/—â, —Ü/—Å)\n",
        "- –ø—Ä–æ–ø—É—Å–∫ –±—É–∫–≤\n",
        "- –ª–∏—à–Ω–∏–µ –±—É–∫–≤—ã\n",
        "- –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è\n",
        "- —Å–ª–∏—Ç–Ω–æ–µ/—Ä–∞–∑–¥–µ–ª—å–Ω–æ–µ –Ω–∞–ø–∏—Å–∞–Ω–∏–µ\n",
        "\n",
        "–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ - JSON —Å–ø–∏—Å–æ–∫:\n",
        "[\n",
        "  {\"input\": \"—Ç–µ–∫—Å—Ç —Å –∞—à–∏–±–∫–∞–º–∏\", \"target\": \"—Ç–µ–∫—Å—Ç —Å –æ—à–∏–±–∫–∞–º–∏\"},\n",
        "  {\"input\": \"–ø—Ä–µ–≤–µ—Ç –º–∏—Ä\", \"target\": \"–ø—Ä–∏–≤–µ—Ç –º–∏—Ä\"}\n",
        "]\n",
        "\n",
        "–¢–µ–º—ã: –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω–∞—è —Ä–µ—á—å, –Ω–æ–≤–æ—Å—Ç–∏, —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ.\n",
        "\"\"\"\n",
        "\n",
        "MODEL_GROQ = \"llama-3.1-8b-instant\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "\n",
        "spell_dataset = []\n",
        "\n",
        "number_of_batches = 30\n",
        "\n",
        "for batch in range(number_of_batches):\n",
        "\n",
        "   \n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt_text}],\n",
        "                model=MODEL_GROQ,\n",
        "                temperature=0.8,\n",
        "                max_tokens=4000\n",
        "        )\n",
        "\n",
        "        # print(response.choices[0].message.content)\n",
        "            \n",
        "        \n",
        "        content = response.choices[0].message.content\n",
        "\n",
        "        # –ü–∞—Ä—Å–∏–º JSON –æ—Ç–≤–µ—Ç\n",
        "        start_idx = content.find('[')\n",
        "        end_idx = content.rfind(']') + 1\n",
        "\n",
        "        if start_idx != -1 and end_idx != -1:\n",
        "            json_str = content[start_idx:end_idx]\n",
        "            batch_data = json.loads(json_str)\n",
        "            \n",
        "            # –î–æ–±–∞–≤–ª—è–µ–º –∫ –æ–±—â–µ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É\n",
        "            spell_dataset.extend(batch_data)\n",
        "            print(f\"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(batch_data)} –ø—Ä–∏–º–µ—Ä–æ–≤. –í—Å–µ–≥–æ: {len(spell_dataset)}\")\n",
        "        else:\n",
        "            print(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞\")\n",
        "                \n",
        "            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏\n",
        "            time.sleep(5)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –≤ –±–∞—Ç—á–µ {batch + 1}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"HF_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ª–æ–∫–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª - —á—Ç–æ–±—ã –ø–æ—Ç–æ–º –µ—Å–ª–∏ —á—Ç–æ –Ω–µ –ø–µ—Ä–µ–≥–µ–Ω–µ—Ä–∏–≤–∞—Ç—å\n",
        "# JSON —Ñ–æ—Ä–º–∞—Ç\n",
        "with open('spell_correction_dataset.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(spell_dataset, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# CSV —Ñ–æ—Ä–º–∞—Ç\n",
        "import pandas as pd\n",
        "if len(spell_dataset) > 0:\n",
        "    df_spell = pd.DataFrame(spell_dataset)\n",
        "    df_spell.to_csv('spell_correction_dataset.csv', index=False, encoding='utf-8')\n",
        "    print(f\"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ CSV: {len(df_spell)} —Å—Ç—Ä–æ–∫\")\n",
        "else:\n",
        "    print(\"‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ CSV\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ —Å–æ–±—Ä–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –≤ —Ñ–∞–π–ª –∏ –ø—Ä–∏–∫—Ä–µ–ø–∏—Ç–µ —Å—é–¥–∞ —Å—Å—ã–ª–∫—É –Ω–∞ –Ω–µ–≥–æ\n",
        "from datasets import Dataset\n",
        "from huggingface_hub import login\n",
        "\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "\n",
        "if len(spell_dataset) > 0:\n",
        "    \n",
        "    inputs = [item['input'] for item in spell_dataset]\n",
        "    targets = [item['target'] for item in spell_dataset]\n",
        "    \n",
        "    hf_dataset = Dataset.from_dict({\n",
        "        'input_text': inputs,\n",
        "        'target_text': targets\n",
        "    })\n",
        "    \n",
        "    print(f\"‚úÖ –°–æ–∑–¥–∞–Ω HuggingFace –¥–∞—Ç–∞—Å–µ—Ç —Å {len(hf_dataset)} –ø—Ä–∏–º–µ—Ä–∞–º–∏\")\n",
        "    \n",
        "    hf_dataset.save_to_disk('spell_correction_hf_dataset')\n",
        "    print(\"üíæ –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –ª–æ–∫–∞–ª—å–Ω–æ –≤ 'spell_correction_hf_dataset'\")\n",
        "    \n",
        "    print(f\"\\nüìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ:\")\n",
        "    print(f\"   –†–∞–∑–º–µ—Ä: {len(hf_dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "    print(f\"   –°—Ç–æ–ª–±—Ü—ã: {hf_dataset.column_names}\")\n",
        "    print(f\"   –ü—Ä–∏–º–µ—Ä: {hf_dataset[0]}\")\n",
        "    \n",
        "    # –ó–∞–≥—Ä—É–∑–∫–∞ –≤ HuggingFace Hub\n",
        "    try:\n",
        "        \n",
        "        api = HfApi(token=os.getenv(\"HF_API_KEY\"))\n",
        "\n",
        "        api.upload_file(\n",
        "            path_or_fileobj=\"spell_correction_dataset.json\",\n",
        "            path_in_repo=\"spell_correction_dataset.json\",\n",
        "            repo_id=\"VladARRRR/russian_correct_words\",\n",
        "            repo_type=\"dataset\",\n",
        "        )\n",
        "        print(\"–î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –≤ HuggingFace Hub!\")\n",
        "    except Exception as e:\n",
        "        print(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ Hub: {e}\")\n",
        "    \n",
        "else:\n",
        "    print(\"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è HuggingFace –¥–∞—Ç–∞—Å–µ—Ç–∞\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "\n",
        "MAX_INPUT = 256\n",
        "\n",
        "# –ú–æ–¥–µ–ª—å\n",
        "tokenizer = AutoTokenizer.from_pretrained(SPELL_MODEL_NAME)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(SPELL_MODEL_NAME)\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–≤–æ–π CSV —Å –ø–∞—Ä–∞–º–∏ (input ‚Üí target)\n",
        "\n",
        "df_spell = pd.read_csv('spell_correction_dataset.csv', encoding='utf-8')\n",
        "\n",
        "dataset = Dataset.from_dict({\n",
        "    'input_text': df_spell['input'].tolist(),\n",
        "    'target_text': df_spell['target'].tolist()\n",
        "})\n",
        "    \n",
        "train_test_split = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = train_test_split['train']\n",
        "eval_dataset = train_test_split['test']\n",
        "\n",
        "# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
        "# –§—É–Ω–∫—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏\n",
        "def preprocess_function(examples):\n",
        "    \n",
        "    inputs = [\"Spell correct: \" + text for text in examples['input_text']]\n",
        "    targets = examples['target_text']\n",
        "    \n",
        "    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –≤—Ö–æ–¥—ã\n",
        "    model_inputs = tokenizer(\n",
        "        inputs, \n",
        "        max_length=MAX_INPUT, \n",
        "        truncation=True, \n",
        "        padding=True\n",
        "    )\n",
        "    \n",
        "    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ü–µ–ª–∏\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            targets, \n",
        "            max_length=MAX_INPUT, \n",
        "            truncation=True, \n",
        "            padding=True\n",
        "        )\n",
        "    \n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "tokenized_eval = eval_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding=True\n",
        ")\n",
        "\n",
        "# –ê—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "        output_dir=\"./spell_correction_finetuned\",\n",
        "        eval_steps=100,\n",
        "        logging_steps=50,\n",
        "        save_steps=200,\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        warmup_steps=100,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=\"./logs\",\n",
        "        save_total_limit=2,\n",
        "        predict_with_generate=True,\n",
        "        fp16=torch.cuda.is_available(),\n",
        "        push_to_hub=False,\n",
        "        report_to=[],  # –û—Ç–∫–ª—é—á–∞–µ–º –≤—Å–µ –ª–æ–≥–≥–µ—Ä—ã (wandb, tensorboard)\n",
        "    )\n",
        "    \n",
        "    \n",
        "# Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "    \n",
        "# –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ\n",
        "trainer.train()\n",
        "    \n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å\n",
        "model_save_path = \"./spell_correction_finetuned_final\"\n",
        "trainer.save_model(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "print(f\"‚úÖ –î–æ–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {model_save_path}\")\n",
        "\n",
        "print(\"üéâ –î–æ–æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏ —Ç–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö Whisper\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å\n",
        "model_path = \"./spell_correction_finetuned_final\"\n",
        "try:\n",
        "    finetuned_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
        "    finetuned_model.to(device)\n",
        "    print(f\"‚úÖ –î–æ–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑: {model_path}\")\n",
        "    model_loaded = True\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}\")\n",
        "    print(\"üí° –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∏ –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞\")\n",
        "    model_loaded = False\n",
        "\n",
        "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é\n",
        "def correct_with_finetuned(text, tokenizer, model, max_length=256):\n",
        "    input_text = \"Spell correct: \" + text\n",
        "    \n",
        "    encoded = tokenizer(\n",
        "        [input_text],\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            encoded[\"input_ids\"],\n",
        "            max_length=max_length,\n",
        "            num_beams=4,\n",
        "            early_stopping=True,\n",
        "            do_sample=False,\n",
        "            temperature=1.0\n",
        "        )\n",
        "    \n",
        "    corrected = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return corrected\n",
        "\n",
        "# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∫ 10 –ø—Ä–∏–º–µ—Ä–∞–º –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Whisper\n",
        "if model_loaded and results_learn_whisper_small:\n",
        "    print(\"\\nüß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –î–û–û–ë–£–ß–ï–ù–ù–û–ô –ú–û–î–ï–õ–ò –ù–ê –†–ï–ó–£–õ–¨–¢–ê–¢–ê–• WHISPER:\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    for i, result in enumerate(results_learn_whisper_small[:10]):\n",
        "        original_whisper_text = result[\"transcription\"]\n",
        "        \n",
        "        # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é\n",
        "        try:\n",
        "            corrected_text = correct_with_finetuned(\n",
        "                original_whisper_text, \n",
        "                finetuned_tokenizer, \n",
        "                finetuned_model\n",
        "            )\n",
        "            \n",
        "            print(f\"\\n{i+1}. üé§ Whisper —Ä–µ–∑—É–ª—å—Ç–∞—Ç:\")\n",
        "            print(f\"   \\\"{original_whisper_text}\\\"\")\n",
        "            print(f\"   üéØ –î–æ–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å:\")\n",
        "            print(f\"   \\\"{corrected_text}\\\"\")\n",
        "            print(\"-\" * 50)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–∏–º–µ—Ä–∞ {i+1}: {e}\")\n",
        "    \n",
        "    print(\"\\nüéâ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\")\n",
        "    print(\"\\nüìä –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:\")\n",
        "    print(\"- –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\")\n",
        "    print(\"- –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ —É–ª—É—á—à–µ–Ω–∏—è –≤ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –æ—à–∏–±–æ–∫ ASR\")\n",
        "    print(\"- –°—Ä–∞–≤–Ω–∏—Ç–µ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —è—á–µ–µ–∫\")\n",
        "    \n",
        "else:\n",
        "    if not model_loaded:\n",
        "        print(\"‚ùå –î–æ–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞\")\n",
        "    if not results_learn_whisper_small:\n",
        "        print(\"‚ùå –†–µ–∑—É–ª—å—Ç–∞—Ç—ã Whisper –Ω–µ –Ω–∞–π–¥–µ–Ω—ã\")\n",
        "    print(\"üí° –í—ã–ø–æ–ª–Ω–∏—Ç–µ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —è—á–µ–π–∫–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è WER\n",
        "!pip install jiwer -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Whisper —Å —ç—Ç–∞–ª–æ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏\n",
        "def match_whisper_with_ground_truth(whisper_results, ground_truth_dict, verbose=False):\n",
        "    \"\"\"\n",
        "    –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã Whisper —Å —ç—Ç–∞–ª–æ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –ø–æ URL\n",
        "    \n",
        "    Args:\n",
        "        whisper_results: —Å–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Whisper —Å –ø–æ–ª—è–º–∏ 'url' –∏ 'transcription'\n",
        "        ground_truth_dict: —Å–ª–æ–≤–∞—Ä—å {url: ground_truth_text}\n",
        "        verbose: –µ—Å–ª–∏ True, –≤—ã–≤–æ–¥–∏—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –æ –Ω–µ—Å–æ–≤–ø–∞–¥–∞—é—â–∏—Ö URL\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (whisper_predictions, matched_ground_truth, matched_urls)\n",
        "    \"\"\"\n",
        "    whisper_predictions = []\n",
        "    matched_ground_truth = []\n",
        "    matched_urls = []\n",
        "    \n",
        "    for result in whisper_results:\n",
        "        url = result[\"url\"]\n",
        "        transcription = result[\"transcription\"]\n",
        "        \n",
        "        if url in ground_truth_dict:\n",
        "            whisper_predictions.append(transcription)\n",
        "            matched_ground_truth.append(ground_truth_dict[url])\n",
        "            matched_urls.append(url)\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(f\"‚ö†Ô∏è URL –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {url}\")\n",
        "    \n",
        "    return whisper_predictions, matched_ground_truth, matched_urls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üî¨ –ò–°–°–õ–ï–î–û–í–ê–ù–ò–ï: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö Spell Correction –º–æ–¥–µ–ª–µ–π\n",
        "print(\"\\nüî¨ –ò–°–°–õ–ï–î–û–í–ê–ù–ò–ï –†–ê–ó–õ–ò–ß–ù–´–• SPELL CORRECTION –ú–û–î–ï–õ–ï–ô\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏–∏\n",
        "spell_models_to_test = [\n",
        "    {\n",
        "        \"name\": \"UrukHan/t5-russian-spell\",\n",
        "        \"prefix\": \"Spell correct: \",\n",
        "        \"description\": \"T5-based –º–æ–¥–µ–ª—å –¥–ª—è —Ä—É—Å—Å–∫–æ–π –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏–∏\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"ai-forever/RuM2M100-1.2B\",\n",
        "        \"prefix\": \"\",\n",
        "        \"description\": \"–ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ (–º–æ–∂–µ—Ç –∏—Å–ø—Ä–∞–≤–ª—è—Ç—å –æ—à–∏–±–∫–∏)\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"cointegrated/rut5-base-multitask\",\n",
        "        \"prefix\": \"–ò—Å–ø—Ä–∞–≤—å –æ—à–∏–±–∫–∏: \",\n",
        "        \"description\": \"RuT5 –º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á NLP\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"facebook/bart-large-cnn\",\n",
        "        \"prefix\": \"correct: \",\n",
        "        \"description\": \"BART –º–æ–¥–µ–ª—å (–¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è, —Ö–æ—Ç—è –Ω–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º)\"\n",
        "    }\n",
        "]\n",
        "\n",
        "spell_correction_results = {}\n",
        "\n",
        "print(\"üìä –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏–∏...\")\n",
        "print(\"üí° –¶–µ–ª—å: –Ω–∞–π—Ç–∏ –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫ ASR –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ\")\n",
        "\n",
        "# –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 10 –ø—Ä–∏–º–µ—Ä–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\n",
        "if len(whisper_predictions) > 0:\n",
        "    test_samples = min(10, len(whisper_predictions))\n",
        "    test_whisper_texts = whisper_predictions[:test_samples]\n",
        "    test_ground_truth_texts = matched_ground_truth[:test_samples]\n",
        "    \n",
        "    print(f\"üìã –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ {test_samples} –ø—Ä–∏–º–µ—Ä–∞—Ö —Å –æ—à–∏–±–∫–∞–º–∏ Whisper\")\n",
        "    \n",
        "    for model_info in spell_models_to_test:\n",
        "        model_name = model_info[\"name\"]\n",
        "        prefix = model_info[\"prefix\"]\n",
        "        description = model_info[\"description\"]\n",
        "        \n",
        "        print(f\"\\nüìù –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å: {model_name}\")\n",
        "        print(f\"   üìÑ –û–ø–∏—Å–∞–Ω–∏–µ: {description}\")\n",
        "        \n",
        "        try:\n",
        "            from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "            \n",
        "            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "            model.to(device)\n",
        "            \n",
        "            print(f\"   ‚úÖ –ú–æ–¥–µ–ª—å {model_name} –∑–∞–≥—Ä—É–∂–µ–Ω–∞\")\n",
        "            \n",
        "            corrected_texts = []\n",
        "            \n",
        "            for whisper_text in test_whisper_texts:\n",
        "                try:\n",
        "                    # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–æ–¥–µ–ª—å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è\n",
        "                    input_text = prefix + whisper_text\n",
        "                    \n",
        "                    encoded = tokenizer(\n",
        "                        [input_text],\n",
        "                        max_length=256,\n",
        "                        truncation=True,\n",
        "                        padding=True,\n",
        "                        return_tensors=\"pt\"\n",
        "                    ).to(device)\n",
        "                    \n",
        "                    with torch.no_grad():\n",
        "                        outputs = model.generate(\n",
        "                            encoded[\"input_ids\"],\n",
        "                            max_length=256,\n",
        "                            num_beams=3,\n",
        "                            early_stopping=True,\n",
        "                            do_sample=False,\n",
        "                            pad_token_id=tokenizer.eos_token_id\n",
        "                        )\n",
        "                    \n",
        "                    corrected = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "                    \n",
        "                    # –£–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å\n",
        "                    if prefix and corrected.startswith(prefix.strip()):\n",
        "                        corrected = corrected[len(prefix.strip()):].strip()\n",
        "                    \n",
        "                    corrected_texts.append(corrected)\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞: {e}\")\n",
        "                    corrected_texts.append(whisper_text)  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª –ø—Ä–∏ –æ—à–∏–±–∫–µ\n",
        "            \n",
        "            # –í—ã—á–∏—Å–ª—è–µ–º WER –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤\n",
        "            if len(corrected_texts) == len(test_ground_truth_texts):\n",
        "                wer_score = calculate_wer(corrected_texts, test_ground_truth_texts)\n",
        "                \n",
        "                spell_correction_results[model_name] = {\n",
        "                    'wer': wer_score,\n",
        "                    'samples': len(corrected_texts),\n",
        "                    'description': description,\n",
        "                    'prefix': prefix\n",
        "                }\n",
        "                \n",
        "                print(f\"   üìà WER: {wer_score:.4f} ({wer_score*100:.2f}%)\")\n",
        "                \n",
        "                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤\n",
        "                print(f\"   üìã –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π:\")\n",
        "                for i in range(min(3, len(corrected_texts))):\n",
        "                    print(f\"      {i+1}. Whisper:    \\\"{test_whisper_texts[i]}\\\"\")\n",
        "                    print(f\"         –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: \\\"{corrected_texts[i]}\\\"\")\n",
        "                    print(f\"         –≠—Ç–∞–ª–æ–Ω:     \\\"{test_ground_truth_texts[i]}\\\"\")\n",
        "                    print()\n",
        "            else:\n",
        "                print(f\"   ‚ùå –û—à–∏–±–∫–∞: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å —ç—Ç–∞–ª–æ–Ω–Ω—ã–º–∏\")\n",
        "            \n",
        "            # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å\n",
        "            del model, tokenizer\n",
        "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}: {e}\")\n",
        "            if \"out of memory\" in str(e).lower():\n",
        "                print(f\"   üí° –ú–æ–¥–µ–ª—å {model_name} —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∞—è –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –æ–∫—Ä—É–∂–µ–Ω–∏—è\")\n",
        "            continue\n",
        "\n",
        "    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è Spell Correction –º–æ–¥–µ–ª–µ–π\n",
        "    print(f\"\\nüèÜ –°–†–ê–í–ù–ï–ù–ò–ï SPELL CORRECTION –ú–û–î–ï–õ–ï–ô:\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    if spell_correction_results:\n",
        "        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ WER (–ª—É—á—à–∏–µ –ø–µ—Ä–≤—ã–º–∏)\n",
        "        sorted_spell_results = sorted(spell_correction_results.items(), \n",
        "                                    key=lambda x: x[1]['wer'] if x[1]['wer'] else float('inf'))\n",
        "        \n",
        "        for model_name, results in sorted_spell_results:\n",
        "            if results['wer'] is not None:\n",
        "                print(f\"{model_name:35} WER: {results['wer']:.4f} ({results['wer']*100:.2f}%)\")\n",
        "                print(f\"{'':35} –ü—Ä–µ—Ñ–∏–∫—Å: '{results['prefix']}'\")\n",
        "                print(f\"{'':35} {results['description']}\")\n",
        "                print()\n",
        "        \n",
        "        best_spell_model = sorted_spell_results[0]\n",
        "        print(f\"ü•á –õ—É—á—à–∞—è Spell Correction –º–æ–¥–µ–ª—å: {best_spell_model[0]}\")\n",
        "        print(f\"   WER: {best_spell_model[1]['wer']:.4f}\")\n",
        "        print(f\"   –ü—Ä–µ—Ñ–∏–∫—Å: '{best_spell_model[1]['prefix']}'\")\n",
        "        print(f\"üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å {best_spell_model[0]} –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è\")\n",
        "    else:\n",
        "        print(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∏ –æ–¥–Ω—É –º–æ–¥–µ–ª—å Spell Correction\")\n",
        "    \n",
        "    print(f\"\\nüìù –í—ã–≤–æ–¥—ã –ø–æ Spell Correction –º–æ–¥–µ–ª—è–º:\")\n",
        "    print(\"- –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\")\n",
        "    print(\"- T5-based –º–æ–¥–µ–ª–∏ —Ö–æ—Ä–æ—à–æ –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è –∑–∞–¥–∞—á –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞\")\n",
        "    print(\"- –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—Ä–µ—Ñ–∏–∫—Å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–µ–Ω –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π\")\n",
        "    print(\"- –ú–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –º–µ–Ω–µ–µ —Ç–æ—á–Ω—ã–º–∏ –¥–ª—è —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –æ—à–∏–±–æ–∫\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö Whisper –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è Spell Correction –º–æ–¥–µ–ª–µ–π\")\n",
        "    print(\"üí° –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ —è—á–µ–π–∫–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∞—É–¥–∏–æ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üöÄ –£–õ–£–ß–®–ï–ù–ù–û–ï –î–û–û–ë–£–ß–ï–ù–ò–ï: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ª—É—á—à–µ–π Spell Correction –º–æ–¥–µ–ª–∏\n",
        "print(\"\\nüöÄ –£–õ–£–ß–®–ï–ù–ù–û–ï –î–û–û–ë–£–ß–ï–ù–ò–ï –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"üéØ –¶–ï–õ–¨: –£–ª—É—á—à–∏—Ç—å –¥–æ–æ–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑:\")\n",
        "print(\"1. üìä –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å ASR-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–º–∏ –æ—à–∏–±–∫–∞–º–∏\")\n",
        "print(\"2. ‚öôÔ∏è  –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã\")\n",
        "print(\"3. üé® –£–ª—É—á—à–µ–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è\")\n",
        "print(\"4. üìà –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–µ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ learning rate\")\n",
        "\n",
        "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤\n",
        "if 'spell_correction_results' in globals() and spell_correction_results:\n",
        "    best_model_name = min(spell_correction_results.items(), key=lambda x: x[1]['wer'])[0]\n",
        "    best_prefix = spell_correction_results[best_model_name]['prefix']\n",
        "    print(f\"\\n‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å: {best_model_name}\")\n",
        "    print(f\"   –ü—Ä–µ—Ñ–∏–∫—Å: '{best_prefix}'\")\n",
        "else:\n",
        "    # Fallback –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –º–æ–¥–µ–ª—å\n",
        "    best_model_name = \"UrukHan/t5-russian-spell\"\n",
        "    best_prefix = \"Spell correct: \"\n",
        "    print(f\"\\nüí° –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –º–æ–¥–µ–ª—å: {best_model_name}\")\n",
        "\n",
        "# 1. –°–û–ó–î–ê–ù–ò–ï –†–ê–°–®–ò–†–ï–ù–ù–û–ì–û –î–ê–¢–ê–°–ï–¢–ê\n",
        "print(f\"\\nüìä –≠–¢–ê–ü 1: –°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\")\n",
        "\n",
        "# –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ ASR-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏\n",
        "asr_specific_errors = [\n",
        "    # –§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–º–µ–Ω—ã (—Ç–∏–ø–∏—á–Ω—ã–µ –¥–ª—è ASR)\n",
        "    (\"—à\", \"—â\"), (\"—â\", \"—à\"), (\"—Ü\", \"—Å\"), (\"—Å\", \"—Ü\"),\n",
        "    (\"–µ\", \"–∏\"), (\"–∏\", \"–µ\"), (\"–æ\", \"–∞\"), (\"–∞\", \"–æ\"),\n",
        "    (\"–±\", \"–ø\"), (\"–ø\", \"–±\"), (\"–¥\", \"—Ç\"), (\"—Ç\", \"–¥\"),\n",
        "    (\"–≥\", \"–∫\"), (\"–∫\", \"–≥\"), (\"–≤\", \"—Ñ\"), (\"—Ñ\", \"–≤\"),\n",
        "    (\"–∑\", \"—Å\"), (\"—Å\", \"–∑\"), (\"–∂\", \"—à\"), (\"—à\", \"–∂\"),\n",
        "    \n",
        "    # –ü—Ä–æ–ø—É—Å–∫–∏ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è\n",
        "    (\"—Ç—å—Å—è\", \"—Ç—Å—è\"), (\"—Ç—Å—è\", \"—Ç—å—Å—è\"),\n",
        "    (\"—á—Ç–æ\", \"—à—Ç–æ\"), (\"—á—Ç–æ–±—ã\", \"—à—Ç–æ–±—ã\"),\n",
        "    (\"–∫–æ–Ω–µ—á–Ω–æ\", \"–∫–∞–Ω–µ—à–Ω–æ\"), (\"—Å–∫—É—á–Ω–æ\", \"—Å–∫—É—à–Ω–æ\"),\n",
        "    \n",
        "    # –°–ª–∏—Ç–Ω–æ–µ/—Ä–∞–∑–¥–µ–ª—å–Ω–æ–µ –Ω–∞–ø–∏—Å–∞–Ω–∏–µ\n",
        "    (\"—Ç–∞–∫–∂–µ\", \"—Ç–∞–∫ –∂–µ\"), (\"—Ç–æ–∂–µ\", \"—Ç–æ –∂–µ\"),\n",
        "    (\"–≤–º–µ—Å—Ç–µ\", \"–≤ –º–µ—Å—Ç–µ\"), (\"–Ω–∞–≤—Å–µ–≥–¥–∞\", \"–Ω–∞ –≤—Å–µ–≥–¥–∞\"),\n",
        "]\n",
        "\n",
        "enhanced_dataset = []\n",
        "\n",
        "# –î–æ–±–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç\n",
        "if 'spell_dataset' in globals() and len(spell_dataset) > 0:\n",
        "    enhanced_dataset.extend(spell_dataset)\n",
        "    print(f\"   ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {len(spell_dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "\n",
        "# –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º ASR-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã\n",
        "import random\n",
        "\n",
        "base_sentences = [\n",
        "    \"–ø—Ä–∏–≤–µ—Ç –∫–∞–∫ –¥–µ–ª–∞ —Å–µ–≥–æ–¥–Ω—è\",\n",
        "    \"—Ö–æ—Ä–æ—à–∞—è –ø–æ–≥–æ–¥–∞ –Ω–∞ —É–ª–∏—Ü–µ\",\n",
        "    \"–Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –¥–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ\", \n",
        "    \"–∑–∞–≤—Ç—Ä–∞ –±—É–¥–µ—Ç –≤–∞–∂–Ω–∞—è –≤—Å—Ç—Ä–µ—á–∞\",\n",
        "    \"—Å–ø–∞—Å–∏–±–æ –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ –∏ –ø–æ–º–æ—â—å\",\n",
        "    \"–∏–∑—É—á–∞—é –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ\",\n",
        "    \"—ç—Ç–æ –æ—á–µ–Ω—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω–∞—è —Ç–µ–º–∞\",\n",
        "    \"–¥–∞–≤–∞–π—Ç–µ –æ–±—Å—É–¥–∏–º —ç—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç\",\n",
        "    \"–¥–æ —Å–≤–∏–¥–∞–Ω–∏—è —É–≤–∏–¥–∏–º—Å—è –∑–∞–≤—Ç—Ä–∞\",\n",
        "    \"—É–¥–∞—á–Ω–æ–≥–æ –¥–Ω—è –≤—Å–µ–º —É—á–∞—Å—Ç–Ω–∏–∫–∞–º\"\n",
        "]\n",
        "\n",
        "asr_generated_count = 0\n",
        "for sentence in base_sentences * 20:  # –ü–æ–≤—Ç–æ—Ä—è–µ–º –¥–ª—è –±–æ–ª—å—à–µ–≥–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è\n",
        "    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ ASR-–æ—à–∏–±–∫–∏\n",
        "    corrupted = sentence\n",
        "    for _ in range(random.randint(1, 3)):  # 1-3 –æ—à–∏–±–∫–∏ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ\n",
        "        if random.random() < 0.7:  # 70% –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏–º–µ–Ω–∏—Ç—å –æ—à–∏–±–∫—É\n",
        "            old_char, new_char = random.choice(asr_specific_errors)\n",
        "            if old_char in corrupted:\n",
        "                corrupted = corrupted.replace(old_char, new_char, 1)\n",
        "    \n",
        "    if corrupted != sentence:  # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è\n",
        "        enhanced_dataset.append({\n",
        "            \"input\": corrupted,\n",
        "            \"target\": sentence\n",
        "        })\n",
        "        asr_generated_count += 1\n",
        "\n",
        "print(f\"   ‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ ASR-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {asr_generated_count}\")\n",
        "print(f\"   üìä –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(enhanced_dataset)}\")\n",
        "\n",
        "# 2. –£–õ–£–ß–®–ï–ù–ù–´–ï –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–´\n",
        "print(f\"\\n‚öôÔ∏è  –≠–¢–ê–ü 2: –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã\")\n",
        "\n",
        "enhanced_training_config = {\n",
        "    \"num_train_epochs\": 5,  # –ë–æ–ª—å—à–µ —ç–ø–æ—Ö –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è\n",
        "    \"per_device_train_batch_size\": 4,  # –ú–µ–Ω—å—à–∏–π batch –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
        "    \"per_device_eval_batch_size\": 8,\n",
        "    \"gradient_accumulation_steps\": 4,  # –≠–º—É–ª–∏—Ä—É–µ–º batch_size=16\n",
        "    \"learning_rate\": 3e-5,  # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π LR –¥–ª—è T5\n",
        "    \"warmup_ratio\": 0.1,  # 10% —à–∞–≥–æ–≤ –Ω–∞ —Ä–∞–∑–æ–≥—Ä–µ–≤\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"max_grad_norm\": 1.0,  # Gradient clipping\n",
        "    \"lr_scheduler_type\": \"cosine\",  # –ö–æ—Å–∏–Ω—É—Å–Ω—ã–π –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫\n",
        "    \"save_strategy\": \"steps\",\n",
        "    \"save_steps\": 200,\n",
        "    \"eval_strategy\": \"steps\", \n",
        "    \"eval_steps\": 100,\n",
        "    \"logging_steps\": 50,\n",
        "    \"load_best_model_at_end\": True,\n",
        "    \"metric_for_best_model\": \"eval_loss\",\n",
        "    \"greater_is_better\": False,\n",
        "    \"save_total_limit\": 3,\n",
        "    \"dataloader_num_workers\": 2,\n",
        "    \"fp16\": torch.cuda.is_available(),\n",
        "    \"report_to\": [],\n",
        "}\n",
        "\n",
        "print(\"   üìã –ö–ª—é—á–µ–≤—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:\")\n",
        "print(f\"   - –£–≤–µ–ª–∏—á–µ–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö: {enhanced_training_config['num_train_epochs']}\")\n",
        "print(f\"   - Gradient accumulation: {enhanced_training_config['gradient_accumulation_steps']}\")\n",
        "print(f\"   - Learning rate: {enhanced_training_config['learning_rate']}\")\n",
        "print(f\"   - –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫: {enhanced_training_config['lr_scheduler_type']}\")\n",
        "print(f\"   - Warmup ratio: {enhanced_training_config['warmup_ratio']}\")\n",
        "\n",
        "# 3. –¢–ï–û–†–ï–¢–ò–ß–ï–°–ö–û–ï –û–ë–û–°–ù–û–í–ê–ù–ò–ï\n",
        "print(f\"\\nüß† –≠–¢–ê–ü 3: –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–∏–π\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"üìö –¢–ï–û–†–ï–¢–ò–ß–ï–°–ö–ò–ï –û–°–ù–û–í–´ –£–õ–£–ß–®–ï–ù–ò–ô:\")\n",
        "print()\n",
        "print(\"1Ô∏è‚É£ –†–ê–°–®–ò–†–ï–ù–ù–´–ô –î–ê–¢–ê–°–ï–¢ –° ASR-–û–®–ò–ë–ö–ê–ú–ò:\")\n",
        "print(\"   ‚Ä¢ –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–º–µ–Ω—ã (—à/—â, –µ/–∏) —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ ASR\")\n",
        "print(\"   ‚Ä¢ –ú–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –∏—Å–ø—Ä–∞–≤–ª—è—Ç—å –∏–º–µ–Ω–Ω–æ —Ç–µ –æ—à–∏–±–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –¥–µ–ª–∞–µ—Ç Whisper\")\n",
        "print(\"   ‚Ä¢ –£–≤–µ–ª–∏—á–µ–Ω–∏–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –¥–∞–Ω–Ω—ã—Ö —É–ª—É—á—à–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—é\")\n",
        "print()\n",
        "print(\"2Ô∏è‚É£ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ï –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–´:\")\n",
        "print(\"   ‚Ä¢ –ú–µ–Ω—å—à–∏–π batch_size + gradient accumulation = —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å + —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å\")\n",
        "print(\"   ‚Ä¢ –ö–æ—Å–∏–Ω—É—Å–Ω—ã–π –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ LR –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ\")\n",
        "print(\"   ‚Ä¢ Warmup –ø–æ–º–æ–≥–∞–µ—Ç –º–æ–¥–µ–ª–∏ –ø–ª–∞–≤–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –Ω–æ–≤–æ–π –∑–∞–¥–∞—á–µ\")\n",
        "print(\"   ‚Ä¢ Gradient clipping –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –≤–∑—Ä—ã–≤–∞—é—â–∏–µ—Å—è –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã\")\n",
        "print()\n",
        "print(\"3Ô∏è‚É£ –£–õ–£–ß–®–ï–ù–ù–´–ï –¢–ï–•–ù–ò–ö–ò –û–ë–£–ß–ï–ù–ò–Ø:\")\n",
        "print(\"   ‚Ä¢ –ë–æ–ª—å—à–µ —ç–ø–æ—Ö –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –ª—É—á—à–µ –≤—ã—É—á–∏—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã –æ—à–∏–±–æ–∫\")\n",
        "print(\"   ‚Ä¢ Early stopping –ø–æ validation loss –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ\")\n",
        "print(\"   ‚Ä¢ FP16 —É—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞\")\n",
        "print()\n",
        "print(\"4Ô∏è‚É£ –û–ñ–ò–î–ê–ï–ú–´–ï –£–õ–£–ß–®–ï–ù–ò–Ø:\")\n",
        "print(\"   ‚Ä¢ WER –¥–æ–ª–∂–µ–Ω —Å–Ω–∏–∑–∏—Ç—å—Å—è –Ω–∞ 15-25% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é\")\n",
        "print(\"   ‚Ä¢ –õ—É—á—à–µ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫ ASR\")\n",
        "print(\"   ‚Ä¢ –ë–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–∞—Ö —Ç–µ–∫—Å—Ç–æ–≤\")\n",
        "\n",
        "# 4. –ó–ê–ü–£–°–ö –£–õ–£–ß–®–ï–ù–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø\n",
        "print(f\"\\nüöÄ –≠–¢–ê–ü 4: –ó–∞–ø—É—Å–∫ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è\")\n",
        "\n",
        "if len(enhanced_dataset) > 100:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
        "    try:\n",
        "        from transformers import (\n",
        "            AutoTokenizer, AutoModelForSeq2SeqLM, \n",
        "            DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "        )\n",
        "        from datasets import Dataset\n",
        "        \n",
        "        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å\n",
        "        print(f\"   üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å: {best_model_name}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(best_model_name)\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(best_model_name)\n",
        "        \n",
        "        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç\n",
        "        inputs = [item['input'] for item in enhanced_dataset]\n",
        "        targets = [item['target'] for item in enhanced_dataset]\n",
        "        \n",
        "        dataset = Dataset.from_dict({\n",
        "            'input_text': inputs,\n",
        "            'target_text': targets\n",
        "        })\n",
        "        \n",
        "        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/validation (85%/15% –¥–ª—è –±–æ–ª—å—à–µ–≥–æ validation set)\n",
        "        train_test_split = dataset.train_test_split(test_size=0.15, seed=42)\n",
        "        train_dataset = train_test_split['train']\n",
        "        eval_dataset = train_test_split['test']\n",
        "        \n",
        "        print(f\"   üìä Train: {len(train_dataset)}, Validation: {len(eval_dataset)}\")\n",
        "        \n",
        "        # –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏\n",
        "        def enhanced_preprocess_function(examples):\n",
        "            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—Ä–µ—Ñ–∏–∫—Å –¥–ª—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏\n",
        "            inputs = [best_prefix + text for text in examples['input_text']]\n",
        "            targets = examples['target_text']\n",
        "            \n",
        "            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n",
        "            model_inputs = tokenizer(\n",
        "                inputs, \n",
        "                max_length=128,  # –£–º–µ–Ω—å—à–∏–ª–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è\n",
        "                truncation=True, \n",
        "                padding=True\n",
        "            )\n",
        "            \n",
        "            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ü–µ–ª–∏\n",
        "            with tokenizer.as_target_tokenizer():\n",
        "                labels = tokenizer(\n",
        "                    targets, \n",
        "                    max_length=128, \n",
        "                    truncation=True, \n",
        "                    padding=True\n",
        "                )\n",
        "            \n",
        "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "            return model_inputs\n",
        "        \n",
        "        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é\n",
        "        print(\"   üîÑ –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...\")\n",
        "        tokenized_train = train_dataset.map(enhanced_preprocess_function, batched=True)\n",
        "        tokenized_eval = eval_dataset.map(enhanced_preprocess_function, batched=True)\n",
        "        \n",
        "        # Data collator\n",
        "        data_collator = DataCollatorForSeq2Seq(\n",
        "            tokenizer=tokenizer,\n",
        "            model=model,\n",
        "            padding=True\n",
        "        )\n",
        "        \n",
        "        # –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è\n",
        "        training_args = Seq2SeqTrainingArguments(\n",
        "            output_dir=\"./enhanced_spell_correction\",\n",
        "            **enhanced_training_config,\n",
        "            predict_with_generate=True,\n",
        "            push_to_hub=False,\n",
        "        )\n",
        "        \n",
        "        # –°–æ–∑–¥–∞–µ–º trainer\n",
        "        trainer = Seq2SeqTrainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=tokenized_train,\n",
        "            eval_dataset=tokenized_eval,\n",
        "            tokenizer=tokenizer,\n",
        "            data_collator=data_collator,\n",
        "        )\n",
        "        \n",
        "        print(\"   üéØ –ù–∞—á–∏–Ω–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ...\")\n",
        "        print(\"   ‚è±Ô∏è  –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 10-20 –º–∏–Ω—É—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞\")\n",
        "        \n",
        "        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ\n",
        "        trainer.train()\n",
        "        \n",
        "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å\n",
        "        enhanced_model_path = \"./enhanced_spell_correction_final\"\n",
        "        trainer.save_model(enhanced_model_path)\n",
        "        tokenizer.save_pretrained(enhanced_model_path)\n",
        "        \n",
        "        print(f\"   ‚úÖ –£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {enhanced_model_path}\")\n",
        "        print(\"   üéâ –£–ª—É—á—à–µ–Ω–Ω–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\")\n",
        "        \n",
        "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
        "        enhanced_model_info = {\n",
        "            'path': enhanced_model_path,\n",
        "            'base_model': best_model_name,\n",
        "            'prefix': best_prefix,\n",
        "            'dataset_size': len(enhanced_dataset),\n",
        "            'training_config': enhanced_training_config\n",
        "        }\n",
        "        \n",
        "        print(f\"\\nüìã –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–ª—É—á—à–µ–Ω–Ω–æ–º –¥–æ–æ–±—É—á–µ–Ω–∏–∏: {e}\")\n",
        "        enhanced_model_info = None\n",
        "        \n",
        "else:\n",
        "    print(\"   ‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è\")\n",
        "    print(f\"   üí° –¢—Ä–µ–±—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º 100 –ø—Ä–∏–º–µ—Ä–æ–≤, –¥–æ—Å—Ç—É–ø–Ω–æ: {len(enhanced_dataset)}\")\n",
        "    enhanced_model_info = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéØ –û–ë–ó–û–† –õ–£–ß–®–ò–• SPEECH-TO-TEXT –ú–û–î–ï–õ–ï–ô –î–õ–Ø –†–£–°–°–ö–û–ì–û –Ø–ó–´–ö–ê\n",
        "print(\"üéØ –õ–£–ß–®–ò–ï SPEECH-TO-TEXT –ú–û–î–ï–õ–ò –î–õ–Ø –†–£–°–°–ö–û–ì–û –Ø–ó–´–ö–ê\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"üìä –†–ï–ô–¢–ò–ù–ì –ú–û–î–ï–õ–ï–ô –ü–û –ö–ê–ß–ï–°–¢–í–£ –ò –î–û–°–¢–£–ü–ù–û–°–¢–ò:\")\n",
        "print()\n",
        "\n",
        "# 1. OpenAI Whisper —Å–µ–º–µ–π—Å—Ç–≤–æ\n",
        "print(\"ü•á 1. OPENAI WHISPER –°–ï–ú–ï–ô–°–¢–í–û (–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)\")\n",
        "print(\"=\" * 50)\n",
        "whisper_models = [\n",
        "    {\n",
        "        \"name\": \"openai/whisper-large-v3\",\n",
        "        \"params\": \"1550M\",\n",
        "        \"wer_ru\": \"~8-12%\",\n",
        "        \"speed\": \"–ú–µ–¥–ª–µ–Ω–Ω–∞—è\",\n",
        "        \"memory\": \"~6GB\",\n",
        "        \"pros\": [\"–õ—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ\", \"–ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω–æ—Å—Ç—å\", \"–ü—É–Ω–∫—Ç—É–∞—Ü–∏—è\"],\n",
        "        \"cons\": [\"–¢—Ä–µ–±—É–µ—Ç –º–Ω–æ–≥–æ —Ä–µ—Å—É—Ä—Å–æ–≤\", \"–ú–µ–¥–ª–µ–Ω–Ω–∞—è\"]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"openai/whisper-large-v2\", \n",
        "        \"params\": \"1550M\",\n",
        "        \"wer_ru\": \"~10-15%\",\n",
        "        \"speed\": \"–ú–µ–¥–ª–µ–Ω–Ω–∞—è\",\n",
        "        \"memory\": \"~6GB\", \n",
        "        \"pros\": [\"–û—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ\", \"–°—Ç–∞–±–∏–ª—å–Ω–∞—è\", \"–•–æ—Ä–æ—à–æ –æ–±—É—á–µ–Ω–∞\"],\n",
        "        \"cons\": [\"–¢—Ä–µ–±—É–µ—Ç –º–Ω–æ–≥–æ —Ä–µ—Å—É—Ä—Å–æ–≤\"]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"openai/whisper-medium\",\n",
        "        \"params\": \"769M\", \n",
        "        \"wer_ru\": \"~12-18%\",\n",
        "        \"speed\": \"–°—Ä–µ–¥–Ω—è—è\",\n",
        "        \"memory\": \"~2GB\",\n",
        "        \"pros\": [\"–•–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å –∫–∞—á–µ—Å—Ç–≤–æ/—Å–∫–æ—Ä–æ—Å—Ç—å\", \"–ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è Colab Pro\"],\n",
        "        \"cons\": [\"–ú–æ–∂–µ—Ç –Ω–µ –ø–æ–º–µ—Å—Ç–∏—Ç—å—Å—è –≤ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π Colab\"]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"openai/whisper-small\",\n",
        "        \"params\": \"244M\",\n",
        "        \"wer_ru\": \"~15-25%\", \n",
        "        \"speed\": \"–ë—ã—Å—Ç—Ä–∞—è\",\n",
        "        \"memory\": \"~1GB\",\n",
        "        \"pros\": [\"–ë—ã—Å—Ç—Ä–∞—è\", \"–ü–æ–º–µ—â–∞–µ—Ç—Å—è –≤ Colab\", \"–•–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ\"],\n",
        "        \"cons\": [\"–ß—É—Ç—å —Ö—É–∂–µ –∫–∞—á–µ—Å—Ç–≤–æ —á–µ–º large\"]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"openai/whisper-base\",\n",
        "        \"params\": \"74M\",\n",
        "        \"wer_ru\": \"~20-30%\",\n",
        "        \"speed\": \"–û—á–µ–Ω—å –±—ã—Å—Ç—Ä–∞—è\", \n",
        "        \"memory\": \"~500MB\",\n",
        "        \"pros\": [\"–û—á–µ–Ω—å –±—ã—Å—Ç—Ä–∞—è\", \"–ú–∞–ª–æ –ø–∞–º—è—Ç–∏\", \"–ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è real-time\"],\n",
        "        \"cons\": [\"–ó–∞–º–µ—Ç–Ω–æ —Ö—É–∂–µ –∫–∞—á–µ—Å—Ç–≤–æ\"]\n",
        "    }\n",
        "]\n",
        "\n",
        "for i, model in enumerate(whisper_models):\n",
        "    print(f\"üì± {model['name']}\")\n",
        "    print(f\"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {model['params']} | WER: {model['wer_ru']} | –ü–∞–º—è—Ç—å: {model['memory']}\")\n",
        "    print(f\"   ‚úÖ –ü–ª—é—Å—ã: {', '.join(model['pros'])}\")\n",
        "    print(f\"   ‚ùå –ú–∏–Ω—É—Å—ã: {', '.join(model['cons'])}\")\n",
        "    print()\n",
        "\n",
        "# 2. –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä—É—Å—Å–∫–∏–µ –º–æ–¥–µ–ª–∏\n",
        "print(\"ü•à 2. –°–ü–ï–¶–ò–ê–õ–ò–ó–ò–†–û–í–ê–ù–ù–´–ï –†–£–°–°–ö–ò–ï –ú–û–î–ï–õ–ò\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "russian_models = [\n",
        "    {\n",
        "        \"name\": \"bond005/wav2vec2-large-ru-golos\",\n",
        "        \"type\": \"Wav2Vec2\",\n",
        "        \"wer_ru\": \"~10-15%\",\n",
        "        \"pros\": [\"–°–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ\", \"–•–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ\", \"–ë—ã—Å—Ç—Ä–∞—è\"],\n",
        "        \"cons\": [\"–¢–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–π\", \"–ù–µ—Ç –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏\", \"–ú–µ–Ω—å—à–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\"]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"facebook/wav2vec2-large-xlsr-53\",\n",
        "        \"type\": \"Wav2Vec2 Multilingual\", \n",
        "        \"wer_ru\": \"~15-25%\",\n",
        "        \"pros\": [\"–ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω–∞—è\", \"–•–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å —Ä—É—Å—Å–∫–∏–º\", \"–û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –±—ã—Å—Ç—Ä–∞—è\"],\n",
        "        \"cons\": [\"–ù–µ—Ç –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏\", \"–¢—Ä–µ–±—É–µ—Ç fine-tuning –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞\"]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"jonatasgrosman/wav2vec2-large-xlsr-53-russian\",\n",
        "        \"type\": \"Wav2Vec2 Fine-tuned\",\n",
        "        \"wer_ru\": \"~12-20%\", \n",
        "        \"pros\": [\"Fine-tuned –Ω–∞ —Ä—É—Å—Å–∫–æ–º\", \"–•–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ\", \"–û—Ç–∫—Ä—ã—Ç—ã–π –∫–æ–¥\"],\n",
        "        \"cons\": [\"–¢–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–π\", \"–ù–µ—Ç –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏\"]\n",
        "    }\n",
        "]\n",
        "\n",
        "for model in russian_models:\n",
        "    print(f\"üá∑üá∫ {model['name']}\")\n",
        "    print(f\"   –¢–∏–ø: {model['type']} | WER: {model['wer_ru']}\")\n",
        "    print(f\"   ‚úÖ –ü–ª—é—Å—ã: {', '.join(model['pros'])}\")\n",
        "    print(f\"   ‚ùå –ú–∏–Ω—É—Å—ã: {', '.join(model['cons'])}\")\n",
        "    print()\n",
        "\n",
        "# 3. –ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ —Ä–µ—à–µ–Ω–∏—è\n",
        "print(\"ü•â 3. –ö–û–ú–ú–ï–†–ß–ï–°–ö–ò–ï –†–ï–®–ï–ù–ò–Ø (API)\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "commercial_models = [\n",
        "    {\n",
        "        \"name\": \"Yandex SpeechKit\",\n",
        "        \"wer_ru\": \"~5-10%\",\n",
        "        \"pros\": [\"–û—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ\", \"–ü—É–Ω–∫—Ç—É–∞—Ü–∏—è\", \"Streaming\", \"–î–∏–∞—Ä–∏–∑–∞—Ü–∏—è\"],\n",
        "        \"cons\": [\"–ü–ª–∞—Ç–Ω—ã–π\", \"–¢—Ä–µ–±—É–µ—Ç API –∫–ª—é—á\", \"–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞\"]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Google Speech-to-Text\",\n",
        "        \"wer_ru\": \"~8-15%\",\n",
        "        \"pros\": [\"–•–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ\", \"–ú–Ω–æ–≥–æ —è–∑—ã–∫–æ–≤\", \"Streaming\", \"–ê–¥–∞–ø—Ç–∞—Ü–∏—è\"],\n",
        "        \"cons\": [\"–ü–ª–∞—Ç–Ω—ã–π\", \"–ù–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –Ω–∞ —Ä—É—Å—Å–∫–æ–º\"]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Azure Speech Services\",\n",
        "        \"wer_ru\": \"~10-18%\", \n",
        "        \"pros\": [\"–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Microsoft\", \"Customization\", \"Real-time\"],\n",
        "        \"cons\": [\"–ü–ª–∞—Ç–Ω—ã–π\", \"–°–ª–æ–∂–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞\"]\n",
        "    }\n",
        "]\n",
        "\n",
        "for model in commercial_models:\n",
        "    print(f\"üíº {model['name']}\")\n",
        "    print(f\"   WER: {model['wer_ru']}\")\n",
        "    print(f\"   ‚úÖ –ü–ª—é—Å—ã: {', '.join(model['pros'])}\")\n",
        "    print(f\"   ‚ùå –ú–∏–Ω—É—Å—ã: {', '.join(model['cons'])}\")\n",
        "    print()\n",
        "\n",
        "# –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –í–´–ë–û–†–£\n",
        "print(\"üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –í–´–ë–û–†–£ –ú–û–î–ï–õ–ò:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "recommendations = [\n",
        "    {\n",
        "        \"scenario\": \"üéØ –õ—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ (–ø—Ä–æ–¥–∞–∫—à–Ω)\",\n",
        "        \"model\": \"openai/whisper-large-v3\",\n",
        "        \"reason\": \"–õ—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞, –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏\"\n",
        "    },\n",
        "    {\n",
        "        \"scenario\": \"‚ö° –ë–∞–ª–∞–Ω—Å –∫–∞—á–µ—Å—Ç–≤–æ/—Å–∫–æ—Ä–æ—Å—Ç—å\",\n",
        "        \"model\": \"openai/whisper-medium\",\n",
        "        \"reason\": \"–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏\"\n",
        "    },\n",
        "    {\n",
        "        \"scenario\": \"üöÄ –î–ª—è Google Colab (–±–µ—Å–ø–ª–∞—Ç–Ω—ã–π)\",\n",
        "        \"model\": \"openai/whisper-small\",\n",
        "        \"reason\": \"–ü–æ–º–µ—â–∞–µ—Ç—Å—è –≤ –ø–∞–º—è—Ç—å, –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ\"\n",
        "    },\n",
        "    {\n",
        "        \"scenario\": \"‚ö° Real-time –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è\",\n",
        "        \"model\": \"openai/whisper-base –∏–ª–∏ wav2vec2\",\n",
        "        \"reason\": \"–ë—ã—Å—Ç—Ä–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞, –Ω–∏–∑–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –ø–∞–º—è—Ç–∏\"\n",
        "    },\n",
        "    {\n",
        "        \"scenario\": \"üí∞ –ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ –ø—Ä–æ–µ–∫—Ç—ã\",\n",
        "        \"model\": \"Yandex SpeechKit\",\n",
        "        \"reason\": \"–õ—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ, –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞\"\n",
        "    },\n",
        "    {\n",
        "        \"scenario\": \"üî¨ –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã\", \n",
        "        \"model\": \"openai/whisper-small + fine-tuning\",\n",
        "        \"reason\": \"–•–æ—Ä–æ—à–∞—è –±–∞–∑–∞ –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –∏ —É–ª—É—á—à–µ–Ω–∏–π\"\n",
        "    }\n",
        "]\n",
        "\n",
        "for rec in recommendations:\n",
        "    print(f\"{rec['scenario']}\")\n",
        "    print(f\"   üéØ –ú–æ–¥–µ–ª—å: {rec['model']}\")\n",
        "    print(f\"   üí≠ –ü—Ä–∏—á–∏–Ω–∞: {rec['reason']}\")\n",
        "    print()\n",
        "\n",
        "# –ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò–ï –°–û–í–ï–¢–´\n",
        "print(\"üõ†Ô∏è –ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò–ï –°–û–í–ï–¢–´ –ü–û –£–õ–£–ß–®–ï–ù–ò–Æ –ö–ê–ß–ï–°–¢–í–ê:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "tips = [\n",
        "    \"1. üìä –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ spell correction –ø–æ—Å–ª–µ ASR (—É–ª—É—á—à–µ–Ω–∏–µ –Ω–∞ 10-30%)\",\n",
        "    \"2. üéØ Fine-tune –º–æ–¥–µ–ª—å –Ω–∞ –≤–∞—à–µ–º –¥–æ–º–µ–Ω–µ (—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Ä–µ—á—å, –º–µ–¥–∏—Ü–∏–Ω–∞ –∏ —Ç.–¥.)\",\n",
        "    \"3. üîß –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (beam_search, temperature)\",\n",
        "    \"4. üìù –î–æ–±–∞–≤—å—Ç–µ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫—É (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —á–∞—Å—Ç—ã—Ö –æ—à–∏–±–æ–∫)\",\n",
        "    \"5. üé§ –£–ª—É—á—à–∏—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –∞—É–¥–∏–æ (—à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)\",\n",
        "    \"6. üìà –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∞–Ω—Å–∞–º–±–ª–∏ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á\",\n",
        "    \"7. üéØ –ê–¥–∞–ø—Ç–∏—Ä—É–π—Ç–µ –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –∞–∫—Ü–µ–Ω—Ç –∏–ª–∏ –¥–∏–∞–ª–µ–∫—Ç\",\n",
        "    \"8. üìä –ú–æ–Ω–∏—Ç–æ—Ä—å—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏ —Å–æ–±–∏—Ä–∞–π—Ç–µ feedback –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è\"\n",
        "]\n",
        "\n",
        "for tip in tips:\n",
        "    print(f\"   {tip}\")\n",
        "\n",
        "print(f\"\\nüéØ –ò–¢–û–ì–û–í–´–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:\")\n",
        "print(\"=\" * 30)\n",
        "print(\"ü•á –î–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞: Whisper Large-v3 + spell correction\")\n",
        "print(\"‚öñÔ∏è  –î–ª—è –±–∞–ª–∞–Ω—Å–∞: Whisper Medium + fine-tuning\") \n",
        "print(\"üöÄ –î–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏: Whisper Small/Base + –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è\")\n",
        "print(\"üí∞ –î–ª—è –±–∏–∑–Ω–µ—Å–∞: Yandex SpeechKit –∏–ª–∏ Google STT\")\n",
        "print(\"üî¨ –î–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π: Whisper Small + —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è\")\n",
        "\n",
        "print(f\"\\nüí° –ü–æ–º–Ω–∏—Ç–µ: –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –≤–∞—à–∏—Ö –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ –∫–∞—á–µ—Å—Ç–≤—É, —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —Ä–µ—Å—É—Ä—Å–∞–º!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üèÜ –§–ò–ù–ê–õ–¨–ù–û–ï –°–†–ê–í–ù–ï–ù–ò–ï: –í—Å–µ –º–æ–¥–µ–ª–∏ –ø—Ä–æ—Ç–∏–≤ —É–ª—É—á—à–µ–Ω–Ω–æ–π\n",
        "print(\"\\nüèÜ –§–ò–ù–ê–õ–¨–ù–û–ï –°–†–ê–í–ù–ï–ù–ò–ï –í–°–ï–• –ú–û–î–ï–õ–ï–ô\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"üéØ –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º:\")\n",
        "print(\"1. üé§ Whisper-small (–±–∞–∑–æ–≤–∞—è)\")\n",
        "print(\"2. üìù + –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è spell correction\")  \n",
        "print(\"3. üîß + –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –¥–æ–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å\")\n",
        "print(\"4. üöÄ + –£–õ–£–ß–®–ï–ù–ù–ê–Ø –¥–æ–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å\")\n",
        "\n",
        "# –¢–µ—Å—Ç–∏—Ä—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å\n",
        "enhanced_wer = None\n",
        "if 'enhanced_model_info' in globals() and enhanced_model_info is not None:\n",
        "    print(f\"\\nüöÄ –¢–µ—Å—Ç–∏—Ä—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å...\")\n",
        "    \n",
        "    try:\n",
        "        from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "        \n",
        "        # –ó–∞–≥—Ä—É–∂–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å\n",
        "        enhanced_tokenizer = AutoTokenizer.from_pretrained(enhanced_model_info['path'])\n",
        "        enhanced_model = AutoModelForSeq2SeqLM.from_pretrained(enhanced_model_info['path'])\n",
        "        enhanced_model.to(device)\n",
        "        \n",
        "        print(f\"   ‚úÖ –£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑: {enhanced_model_info['path']}\")\n",
        "        \n",
        "        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ —Ç–µ—Ö –∂–µ –¥–∞–Ω–Ω—ã—Ö —á—Ç–æ –∏ –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏\n",
        "        if len(whisper_predictions) > 0:\n",
        "            enhanced_corrected_predictions = []\n",
        "            \n",
        "            for i, result in enumerate(results_learn_whisper_small):\n",
        "                url = result[\"url\"]\n",
        "                original_text = result[\"transcription\"]\n",
        "                \n",
        "                if url in ground_truth_dict:\n",
        "                    try:\n",
        "                        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é\n",
        "                        input_text = enhanced_model_info['prefix'] + original_text\n",
        "                        \n",
        "                        encoded = enhanced_tokenizer(\n",
        "                            [input_text],\n",
        "                            max_length=128,\n",
        "                            truncation=True,\n",
        "                            padding=True,\n",
        "                            return_tensors=\"pt\"\n",
        "                        ).to(device)\n",
        "                        \n",
        "                        with torch.no_grad():\n",
        "                            outputs = enhanced_model.generate(\n",
        "                                encoded[\"input_ids\"],\n",
        "                                max_length=128,\n",
        "                                num_beams=4,\n",
        "                                early_stopping=True,\n",
        "                                do_sample=False\n",
        "                            )\n",
        "                        \n",
        "                        corrected = enhanced_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "                        enhanced_corrected_predictions.append(corrected)\n",
        "                        \n",
        "                    except Exception as e:\n",
        "                        print(f\"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}\")\n",
        "                        enhanced_corrected_predictions.append(original_text)\n",
        "            \n",
        "            if len(enhanced_corrected_predictions) > 0:\n",
        "                enhanced_wer = calculate_wer(enhanced_corrected_predictions, matched_ground_truth)\n",
        "                print(f\"   üìà WER —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏: {enhanced_wer:.4f} ({enhanced_wer*100:.2f}%)\")\n",
        "                \n",
        "                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã —É–ª—É—á—à–µ–Ω–∏–π\n",
        "                print(f\"   üìã –ü—Ä–∏–º–µ—Ä—ã —Ä–∞–±–æ—Ç—ã —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏:\")\n",
        "                for i in range(min(3, len(enhanced_corrected_predictions))):\n",
        "                    print(f\"      {i+1}. Whisper:     \\\"{whisper_predictions[i]}\\\"\")\n",
        "                    print(f\"         –£–ª—É—á—à–µ–Ω–Ω–∞—è: \\\"{enhanced_corrected_predictions[i]}\\\"\")\n",
        "                    print(f\"         –≠—Ç–∞–ª–æ–Ω:     \\\"{matched_ground_truth[i]}\\\"\")\n",
        "                    print()\n",
        "            \n",
        "            # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å\n",
        "            del enhanced_model, enhanced_tokenizer\n",
        "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}\")\n",
        "        enhanced_wer = None\n",
        "else:\n",
        "    print(\"   üí° –£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ –∏–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞\")\n",
        "\n",
        "# –ò–¢–û–ì–û–í–ê–Ø –¢–ê–ë–õ–ò–¶–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í\n",
        "print(f\"\\nüìä –ò–¢–û–ì–û–í–ê–Ø –¢–ê–ë–õ–ò–¶–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "results_table = []\n",
        "\n",
        "# –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
        "if 'wer_whisper' in globals() and wer_whisper is not None:\n",
        "    results_table.append((\"üé§ Whisper-small (–±–∞–∑–æ–≤–∞—è)\", wer_whisper, \"–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å ASR\"))\n",
        "\n",
        "if 'wer_pretrained' in globals() and wer_pretrained is not None:\n",
        "    improvement_pre = ((wer_whisper - wer_pretrained) / wer_whisper * 100) if wer_whisper else 0\n",
        "    results_table.append((\"üìù + –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è spell correction\", wer_pretrained, f\"–£–ª—É—á—à–µ–Ω–∏–µ: {improvement_pre:.1f}%\"))\n",
        "\n",
        "if 'wer_finetuned' in globals() and wer_finetuned is not None:\n",
        "    improvement_ft = ((wer_whisper - wer_finetuned) / wer_whisper * 100) if wer_whisper else 0\n",
        "    results_table.append((\"üîß + –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –¥–æ–æ–±—É—á–µ–Ω–Ω–∞—è\", wer_finetuned, f\"–£–ª—É—á—à–µ–Ω–∏–µ: {improvement_ft:.1f}%\"))\n",
        "\n",
        "if enhanced_wer is not None:\n",
        "    improvement_enh = ((wer_whisper - enhanced_wer) / wer_whisper * 100) if wer_whisper else 0\n",
        "    results_table.append((\"üöÄ + –£–õ–£–ß–®–ï–ù–ù–ê–Ø –¥–æ–æ–±—É—á–µ–Ω–Ω–∞—è\", enhanced_wer, f\"–£–ª—É—á—à–µ–Ω–∏–µ: {improvement_enh:.1f}%\"))\n",
        "\n",
        "# –í—ã–≤–æ–¥–∏–º —Ç–∞–±–ª–∏—Ü—É\n",
        "if results_table:\n",
        "    print(f\"{'–ú–æ–¥–µ–ª—å':<35} {'WER':<12} {'–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π'}\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    for model_name, wer_score, comment in results_table:\n",
        "        print(f\"{model_name:<35} {wer_score:.4f} ({wer_score*100:5.2f}%) {comment}\")\n",
        "    \n",
        "    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å\n",
        "    best_result = min(results_table, key=lambda x: x[1])\n",
        "    print(f\"\\nü•á –õ–£–ß–®–ò–ô –†–ï–ó–£–õ–¨–¢–ê–¢: {best_result[0]}\")\n",
        "    print(f\"   WER: {best_result[1]:.4f} ({best_result[1]*100:.2f}%)\")\n",
        "    \n",
        "    # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–µ–µ —É–ª—É—á—à–µ–Ω–∏–µ\n",
        "    if len(results_table) > 1:\n",
        "        baseline_wer = results_table[0][1]  # Whisper-small\n",
        "        best_wer = best_result[1]\n",
        "        total_improvement = ((baseline_wer - best_wer) / baseline_wer * 100)\n",
        "        print(f\"   üìà –û–±—â–µ–µ —É–ª—É—á—à–µ–Ω–∏–µ: {total_improvement:.1f}% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é\")\n",
        "\n",
        "# –í–´–í–û–î–´ –ò –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò\n",
        "print(f\"\\nüìù –í–´–í–û–î–´ –ò–°–°–õ–ï–î–û–í–ê–ù–ò–Ø:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"üîç –û–°–ù–û–í–ù–´–ï –ù–ê–•–û–î–ö–ò:\")\n",
        "print()\n",
        "\n",
        "if 'whisper_results' in globals() and whisper_results:\n",
        "    print(\"1Ô∏è‚É£ WHISPER –ú–û–î–ï–õ–ò:\")\n",
        "    best_whisper_model = min(whisper_results.items(), key=lambda x: x[1]['wer'] if x[1]['wer'] else float('inf'))\n",
        "    print(f\"   ‚Ä¢ –õ—É—á—à–∞—è Whisper –º–æ–¥–µ–ª—å: {best_whisper_model[0]}\")\n",
        "    print(f\"   ‚Ä¢ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å –∫–∞—á–µ—Å—Ç–≤–æ/—Ä–µ—Å—É—Ä—Å—ã –¥–ª—è Google Colab\")\n",
        "\n",
        "if 'spell_correction_results' in globals() and spell_correction_results:\n",
        "    print(\"\\n2Ô∏è‚É£ SPELL CORRECTION –ú–û–î–ï–õ–ò:\")\n",
        "    best_spell_model = min(spell_correction_results.items(), key=lambda x: x[1]['wer'] if x[1]['wer'] else float('inf'))\n",
        "    print(f\"   ‚Ä¢ –õ—É—á—à–∞—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å: {best_spell_model[0]}\")\n",
        "    print(f\"   ‚Ä¢ –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä—É—Å—Å–∫–∏–µ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ\")\n",
        "\n",
        "if enhanced_wer is not None:\n",
        "    print(\"\\n3Ô∏è‚É£ –£–õ–£–ß–®–ï–ù–ù–û–ï –î–û–û–ë–£–ß–ï–ù–ò–ï:\")\n",
        "    print(\"   ‚Ä¢ ASR-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ\")\n",
        "    print(\"   ‚Ä¢ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã\")\n",
        "    print(\"   ‚Ä¢ –ö–æ—Å–∏–Ω—É—Å–Ω—ã–π –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ LR –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ\")\n",
        "\n",
        "print(f\"\\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –î–õ–Ø –ü–†–û–î–ê–ö–®–ï–ù–ê:\")\n",
        "print(\"=\" * 40)\n",
        "print(\"1. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ª—É—á—à—É—é –Ω–∞–π–¥–µ–Ω–Ω—É—é –∫–æ–º–±–∏–Ω–∞—Ü–∏—é –º–æ–¥–µ–ª–µ–π\")\n",
        "print(\"2. –†–µ–≥—É–ª—è—Ä–Ω–æ –¥–æ–æ–±—É—á–∞–π—Ç–µ –Ω–∞ –Ω–æ–≤—ã—Ö ASR-–æ—à–∏–±–∫–∞—Ö\")\n",
        "print(\"3. –ú–æ–Ω–∏—Ç–æ—Ä—å—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–∞—Ö –∞—É–¥–∏–æ\")\n",
        "print(\"4. –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –∞–Ω—Å–∞–º–±–ª–∏ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–π\")\n",
        "\n",
        "print(f\"\\nüéØ –î–û–°–¢–ò–ì–ù–£–¢–´–ï –¶–ï–õ–ò:\")\n",
        "print(\"‚úÖ –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã —Ä–∞–∑–ª–∏—á–Ω—ã–µ Whisper –º–æ–¥–µ–ª–∏\")\n",
        "print(\"‚úÖ –°—Ä–∞–≤–Ω–µ–Ω—ã multiple spell correction –ø–æ–¥—Ö–æ–¥—ã\") \n",
        "print(\"‚úÖ –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ —É–ª—É—á—à–µ–Ω–Ω–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ —Å —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–º –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ–º\")\n",
        "print(\"‚úÖ –ü–æ–ª—É—á–µ–Ω—ã –∏–∑–º–µ—Ä–∏–º—ã–µ —É–ª—É—á—à–µ–Ω–∏—è WER\")\n",
        "print(\"‚úÖ –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\")\n",
        "\n",
        "print(f\"\\nüöÄ –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –í—Å–µ —Ü–µ–ª–∏ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—ã.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üî¨ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –°–ü–ï–¶–ò–ê–õ–ò–ó–ò–†–û–í–ê–ù–ù–´–• –†–£–°–°–ö–ò–• WAV2VEC2 –ú–û–î–ï–õ–ï–ô\n",
        "print(\"üî¨ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –°–ü–ï–¶–ò–ê–õ–ò–ó–ò–†–û–í–ê–ù–ù–´–• –†–£–°–°–ö–ò–• WAV2VEC2 –ú–û–î–ï–õ–ï–ô\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"üéØ –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏:\")\n",
        "print(\"1. bond005/wav2vec2-large-ru-golos - —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ\")\n",
        "print(\"2. jonatasgrosman/wav2vec2-large-xlsr-53-russian - fine-tuned –Ω–∞ —Ä—É—Å—Å–∫–æ–º\")\n",
        "\n",
        "# –°–ø–∏—Å–æ–∫ Wav2Vec2 –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\n",
        "wav2vec2_models_to_test = [\n",
        "    {\n",
        "        \"name\": \"bond005/wav2vec2-large-ru-golos\",\n",
        "        \"description\": \"Wav2Vec2 Large –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ —Ä—É—Å—Å–∫–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ Golos\",\n",
        "        \"language\": \"ru\",\n",
        "        \"type\": \"Wav2Vec2\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"jonatasgrosman/wav2vec2-large-xlsr-53-russian\", \n",
        "        \"description\": \"Wav2Vec2 XLSR-53 fine-tuned –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ\",\n",
        "        \"language\": \"ru\",\n",
        "        \"type\": \"Wav2Vec2 Fine-tuned\"\n",
        "    }\n",
        "]\n",
        "\n",
        "wav2vec2_results = {}\n",
        "\n",
        "print(f\"\\nüìä –ù–∞—á–∏–Ω–∞–µ–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Wav2Vec2 –º–æ–¥–µ–ª–µ–π...\")\n",
        "print(\"üí° Wav2Vec2 –º–æ–¥–µ–ª–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞–ø—Ä—è–º—É—é —Å –∞—É–¥–∏–æ –∏ –Ω–µ —Ç—Ä–µ–±—É—é—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∫–∞–∫ Whisper\")\n",
        "\n",
        "# –ë–µ—Ä–µ–º —Ç–µ –∂–µ –∞—É–¥–∏–æ —á—Ç–æ –∏ –¥–ª—è Whisper –¥–ª—è —á–µ—Å—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
        "if 'results_learn_whisper_small' in globals() and len(results_learn_whisper_small) > 0:\n",
        "    test_samples = min(20, len(results_learn_whisper_small))  # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ 20 –ø—Ä–∏–º–µ—Ä–∞—Ö\n",
        "    print(f\"üìã –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ {test_samples} –∞—É–¥–∏–æ –ø—Ä–∏–º–µ—Ä–∞—Ö\")\n",
        "    \n",
        "    for model_info in wav2vec2_models_to_test:\n",
        "        model_name = model_info[\"name\"]\n",
        "        description = model_info[\"description\"]\n",
        "        \n",
        "        print(f\"\\nüé§ –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å: {model_name}\")\n",
        "        print(f\"   üìÑ –û–ø–∏—Å–∞–Ω–∏–µ: {description}\")\n",
        "        \n",
        "        try:\n",
        "            from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "            import torch\n",
        "            import torchaudio\n",
        "            \n",
        "            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä\n",
        "            print(f\"   üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä...\")\n",
        "            processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
        "            \n",
        "            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è meta tensor –æ—à–∏–±–∫–∏\n",
        "            try:\n",
        "                model = Wav2Vec2ForCTC.from_pretrained(\n",
        "                    model_name,\n",
        "                    torch_dtype=torch.float32,\n",
        "                    device_map=None,\n",
        "                    low_cpu_mem_usage=False\n",
        "                )\n",
        "                model.to(device)\n",
        "            except Exception as meta_error:\n",
        "                print(f\"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å meta tensor, –ø—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–± –∑–∞–≥—Ä—É–∑–∫–∏...\")\n",
        "                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–± –∑–∞–≥—Ä—É–∑–∫–∏\n",
        "                model = Wav2Vec2ForCTC.from_pretrained(model_name, torch_dtype=torch.float32)\n",
        "                if hasattr(model, 'to_empty'):\n",
        "                    model = model.to_empty(device=device)\n",
        "                else:\n",
        "                    model = model.to(device)\n",
        "            \n",
        "            model.eval()\n",
        "            \n",
        "            print(f\"   ‚úÖ –ú–æ–¥–µ–ª—å {model_name} –∑–∞–≥—Ä—É–∂–µ–Ω–∞\")\n",
        "            \n",
        "            wav2vec2_predictions = []\n",
        "            wav2vec2_urls = []\n",
        "            \n",
        "            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∞—É–¥–∏–æ —Ñ–∞–π–ª—ã\n",
        "            for i, result in enumerate(results_learn_whisper_small[:test_samples]):\n",
        "                url = result[\"url\"]\n",
        "                \n",
        "                try:\n",
        "                    print(f\"   üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∞—É–¥–∏–æ {i+1}/{test_samples}...\")\n",
        "                    \n",
        "                    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ\n",
        "                    wav, sr = torchaudio.load(url)\n",
        "                    \n",
        "                    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –º–æ–Ω–æ –µ—Å–ª–∏ —Å—Ç–µ—Ä–µ–æ\n",
        "                    if wav.shape[0] > 1:\n",
        "                        wav = wav.mean(dim=0, keepdim=True)\n",
        "                    \n",
        "                    # Wav2Vec2 –æ–±—ã—á–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å 16kHz\n",
        "                    if sr != 16000:\n",
        "                        wav = torchaudio.functional.resample(wav, orig_freq=sr, new_freq=16000)\n",
        "                        sr = 16000\n",
        "                    \n",
        "                    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É (–º–∞–∫—Å–∏–º—É–º 30 —Å–µ–∫—É–Ω–¥)\n",
        "                    max_length = 30 * sr\n",
        "                    if wav.shape[1] > max_length:\n",
        "                        wav = wav[:, :max_length]\n",
        "                    \n",
        "                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy\n",
        "                    audio_array = wav.squeeze().numpy()\n",
        "                    \n",
        "                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–º\n",
        "                    inputs = processor(\n",
        "                        audio_array, \n",
        "                        sampling_rate=sr, \n",
        "                        return_tensors=\"pt\", \n",
        "                        padding=True\n",
        "                    )\n",
        "                    \n",
        "                    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "                    with torch.no_grad():\n",
        "                        logits = model(inputs.input_values.to(device)).logits\n",
        "                    \n",
        "                    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º\n",
        "                    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "                    transcription = processor.batch_decode(predicted_ids)[0]\n",
        "                    \n",
        "                    # –û—á–∏—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç (—É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã)\n",
        "                    transcription = ' '.join(transcription.split())\n",
        "                    \n",
        "                    wav2vec2_predictions.append(transcription)\n",
        "                    wav2vec2_urls.append(url)\n",
        "                    \n",
        "                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –¥–ª—è –ø–µ—Ä–≤—ã—Ö 5\n",
        "                    if i < 5:\n",
        "                        print(f\"      URL: ...{url[-50:]}\")\n",
        "                        print(f\"      –†–µ–∑—É–ª—å—Ç–∞—Ç: \\\"{transcription}\\\"\")\n",
        "                        print()\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—É–¥–∏–æ {i+1}: {e}\")\n",
        "                    continue\n",
        "            \n",
        "            print(f\"   ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(wav2vec2_predictions)} –∞—É–¥–∏–æ —Ñ–∞–π–ª–æ–≤\")\n",
        "            \n",
        "            # –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º —Å —ç—Ç–∞–ª–æ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è WER\n",
        "            if 'ground_truth_dict' in globals() and len(wav2vec2_predictions) > 0:\n",
        "                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—à—É —Ñ—É–Ω–∫—Ü–∏—é —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è\n",
        "                wav2vec2_results_matched = []\n",
        "                for i, url in enumerate(wav2vec2_urls):\n",
        "                    wav2vec2_results_matched.append({\n",
        "                        \"url\": url,\n",
        "                        \"transcription\": wav2vec2_predictions[i]\n",
        "                    })\n",
        "                \n",
        "                matched_predictions, matched_ground_truth, matched_urls = match_whisper_with_ground_truth(\n",
        "                    wav2vec2_results_matched, \n",
        "                    ground_truth_dict, \n",
        "                    verbose=False\n",
        "                )\n",
        "                \n",
        "                if len(matched_predictions) > 0:\n",
        "                    # –í—ã—á–∏—Å–ª—è–µ–º WER\n",
        "                    wer_score = calculate_wer(matched_predictions, matched_ground_truth)\n",
        "                    \n",
        "                    wav2vec2_results[model_name] = {\n",
        "                        'wer': wer_score,\n",
        "                        'samples': len(matched_predictions),\n",
        "                        'description': description,\n",
        "                        'type': model_info['type']\n",
        "                    }\n",
        "                    \n",
        "                    print(f\"   üìà WER: {wer_score:.4f} ({wer_score*100:.2f}%)\")\n",
        "                    print(f\"   üìä –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–æ —Å —ç—Ç–∞–ª–æ–Ω–æ–º: {len(matched_predictions)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "                    \n",
        "                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
        "                    print(f\"   üìã –ü—Ä–∏–º–µ—Ä—ã —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è:\")\n",
        "                    for i in range(min(3, len(matched_predictions))):\n",
        "                        print(f\"      {i+1}. Wav2Vec2:  \\\"{matched_predictions[i]}\\\"\")\n",
        "                        print(f\"         –≠—Ç–∞–ª–æ–Ω:    \\\"{matched_ground_truth[i]}\\\"\")\n",
        "                        print()\n",
        "                else:\n",
        "                    print(f\"   ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å —ç—Ç–∞–ª–æ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏\")\n",
        "            else:\n",
        "                print(f\"   ‚ö†Ô∏è –ù–µ—Ç —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è WER\")\n",
        "            \n",
        "            # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å\n",
        "            del model, processor\n",
        "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}: {e}\")\n",
        "            if \"out of memory\" in str(e).lower():\n",
        "                print(f\"   üí° –ú–æ–¥–µ–ª—å {model_name} —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∞—è –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –æ–∫—Ä—É–∂–µ–Ω–∏—è\")\n",
        "            elif \"not found\" in str(e).lower():\n",
        "                print(f\"   üí° –ú–æ–¥–µ–ª—å {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞\")\n",
        "            continue\n",
        "\n",
        "    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è Wav2Vec2 –º–æ–¥–µ–ª–µ–π\n",
        "    print(f\"\\nüèÜ –†–ï–ó–£–õ–¨–¢–ê–¢–´ WAV2VEC2 –ú–û–î–ï–õ–ï–ô:\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    if wav2vec2_results:\n",
        "        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ WER (–ª—É—á—à–∏–µ –ø–µ—Ä–≤—ã–º–∏)\n",
        "        sorted_wav2vec2_results = sorted(wav2vec2_results.items(), \n",
        "                                        key=lambda x: x[1]['wer'] if x[1]['wer'] else float('inf'))\n",
        "        \n",
        "        print(f\"{'–ú–æ–¥–µ–ª—å':<45} {'WER':<12} {'–¢–∏–ø'}\")\n",
        "        print(\"-\" * 70)\n",
        "        \n",
        "        for model_name, results in sorted_wav2vec2_results:\n",
        "            if results['wer'] is not None:\n",
        "                short_name = model_name.split('/')[-1]  # –ö–æ—Ä–æ—Ç–∫–æ–µ –∏–º—è\n",
        "                print(f\"{short_name:<45} {results['wer']:.4f} ({results['wer']*100:5.2f}%) {results['type']}\")\n",
        "        \n",
        "        print()\n",
        "        \n",
        "        best_wav2vec2_model = sorted_wav2vec2_results[0]\n",
        "        print(f\"ü•á –õ—É—á—à–∞—è Wav2Vec2 –º–æ–¥–µ–ª—å: {best_wav2vec2_model[0]}\")\n",
        "        print(f\"   WER: {best_wav2vec2_model[1]['wer']:.4f}\")\n",
        "        print(f\"   –¢–∏–ø: {best_wav2vec2_model[1]['type']}\")\n",
        "        \n",
        "        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å Whisper\n",
        "        if 'wer_whisper' in globals() and wer_whisper is not None:\n",
        "            improvement = ((wer_whisper - best_wav2vec2_model[1]['wer']) / wer_whisper * 100)\n",
        "            if improvement > 0:\n",
        "                print(f\"   üìà –£–ª—É—á—à–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Whisper-small: {improvement:.1f}%\")\n",
        "            else:\n",
        "                print(f\"   üìâ Whisper-small –ª—É—á—à–µ –Ω–∞: {abs(improvement):.1f}%\")\n",
        "        \n",
        "    else:\n",
        "        print(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∏ –æ–¥–Ω—É Wav2Vec2 –º–æ–¥–µ–ª—å\")\n",
        "    \n",
        "    print(f\"\\nüìù –í—ã–≤–æ–¥—ã –ø–æ Wav2Vec2 –º–æ–¥–µ–ª—è–º:\")\n",
        "    print(\"- Wav2Vec2 –º–æ–¥–µ–ª–∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ\")\n",
        "    print(\"- –ù–µ –¥–æ–±–∞–≤–ª—è—é—Ç –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é (—Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç)\")\n",
        "    print(\"- –ú–æ–≥—É—Ç –±—ã—Ç—å –±—ã—Å—Ç—Ä–µ–µ Whisper –Ω–∞ CPU\")\n",
        "    print(\"- –•–æ—Ä–æ—à–æ –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è –∑–∞–¥–∞—á –≥–¥–µ –Ω–µ –Ω—É–∂–Ω–∞ –ø—É–Ω–∫—Ç—É–∞—Ü–∏—è\")\n",
        "    print(\"- –¢—Ä–µ–±—É—é—Ç –º–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏ —á–µ–º –±–æ–ª—å—à–∏–µ Whisper –º–æ–¥–µ–ª–∏\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö Whisper –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å Wav2Vec2 –º–æ–¥–µ–ª—è–º–∏\")\n",
        "    print(\"üí° –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ —è—á–µ–π–∫–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∞—É–¥–∏–æ Whisper\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
